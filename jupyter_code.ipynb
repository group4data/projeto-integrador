{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pyodbc\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Iniciando com Spark\") \\\n",
    "    .config('spark.ui.port', '4051') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = \"Data/Clients\"\n",
    "\n",
    "transactions_in = \"Data/Transactions-in\"\n",
    "transactions_out = \"Data/Transactions-out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"nome\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"data_cadastro\", TimestampType(), True),\n",
    "        StructField(\"telefone\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "transactions_schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"cliente_id\", IntegerType(), True),\n",
    "        StructField(\"valor\", DoubleType(), True),\n",
    "        StructField(\"data\", TimestampType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_csv_to_df(spark, path, schema):\n",
    "    if not os.path.isdir(path):\n",
    "        raise ValueError(f\"{path} não é um diretório válido.\")\n",
    "\n",
    "    list_paths_csv = glob.glob(os.path.join(path, '*.csv'))\n",
    "\n",
    "    if not list_paths_csv:\n",
    "        raise ValueError(f\"Não foram encontrados arquivos csv em {path}.\")\n",
    "\n",
    "    df = spark.read.csv(list_paths_csv, sep=';', schema=schema, inferSchema=True)\n",
    "\n",
    "    df = df.filter(~col('id').contains('id'))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_empty_data(df):\n",
    "    for col_name in df.columns:\n",
    "        data_type = df.schema[col_name].dataType\n",
    "        if data_type == StringType():\n",
    "            count_empty = df.filter((col(col_name) == '') | isnull(col_name) | isnan(col_name) | (col(col_name).isNull())).count()\n",
    "            if count_empty != 0:\n",
    "                print(f\"Column '{col_name}' has {count_empty} empty/null/none/NaN values.\")\n",
    "        elif data_type == IntegerType():\n",
    "            count_null = df.filter(col(col_name).isNull()).count()\n",
    "            if count_null != 0:\n",
    "                print(f\"Column '{col_name}' has {count_null} null values.\")\n",
    "        elif data_type == TimestampType():\n",
    "            count_null = df.filter(col(col_name).isNull()).count()\n",
    "            if count_null != 0:\n",
    "                print(f\"Column '{col_name}' has {count_null} null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correcting_data(df):\n",
    "    df = df.withColumn(\"valor\", round(col(\"valor\"), 2))\n",
    "    df = df.withColumn('valor', expr('abs(valor)'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_column(df):\n",
    "    # Cria coluna do DDD a partir da coluna Telefone\n",
    "    df = df.withColumn('DDD', split(df['telefone'], r'[()]+').getItem(1))\n",
    "    # Substitui os DDDs pelo Estado correspondente\n",
    "    df = df.withColumn('estado', when(col('DDD') == '20', 'Paraíba')\n",
    "                        .when(col('DDD') == '21', 'Rio de Janeiro')\n",
    "                        .when(col('DDD') == '22', 'Mato Grosso')\n",
    "                        .when(col('DDD') == '23', 'Pernambuco')\n",
    "                        .when(col('DDD') == '24', 'Rio de Janeiro')\n",
    "                        .when(col('DDD') == '25', 'Bahia')\n",
    "                        .when(col('DDD') == '26', 'Minas Gerais')\n",
    "                        .when(col('DDD') == '27', 'Espírito Santo')\n",
    "                        .when(col('DDD') == '28', 'Roraima')\n",
    "                        .when(col('DDD') == '29', 'São Paulo')\n",
    "                        .when(col('DDD') == '30', 'Maranhão')\n",
    "                        .otherwise('Inválido'))\n",
    "    # Apagando coluna DDD\n",
    "    df = df.drop('DDD')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_names(df):\n",
    "    # Separando os nomes dos sobrenomes\n",
    "    df = df.withColumn(\"nome_split\", split(df.nome, \" \"))\n",
    "    df = df.withColumn(\"nome\", df.nome_split[0])\n",
    "    df = df.withColumn(\"sobrenome1\", df.nome_split[1])\n",
    "    df = df.withColumn(\"sobrenome2\", df.nome_split[2])\n",
    "    df = df.withColumn(\"sobrenome3\", df.nome_split[3])\n",
    "    df = df.withColumn(\"sobrenome4\", df.nome_split[4])\n",
    "    df = df.withColumn(\"sobrenome5\", df.nome_split[5])\n",
    "    df = df.withColumn(\"sobrenome6\", df.nome_split[6])\n",
    "    df = df.withColumn(\"sobrenome\", concat_ws(\" \", \"sobrenome1\", \"sobrenome2\", \"sobrenome3\", \"sobrenome4\", \"sobrenome5\", \"sobrenome6\"))\n",
    "    df = df.drop(\"nome_split\", \"sobrenome1\", \"sobrenome2\", \"sobrenome3\", \"sobrenome4\", \"sobrenome5\", \"sobrenome6\")\n",
    "    # Colocando primeira letra de cada nome Maiúscula\n",
    "    df = df.withColumn(\"nome\", initcap(df.nome))\n",
    "    df = df.withColumn(\"sobrenome\", initcap(df.sobrenome))\n",
    "    # Substituindo linhas vazias por Não Informado\n",
    "    df = df.withColumn(\"sobrenome\", when(df.sobrenome == \"\", \"Não informado\").otherwise(df.sobrenome))\n",
    "    # Ordenando as colunas\n",
    "    df = df.select(\"id\", \"nome\", \"sobrenome\", \"email\", \"data_cadastro\", \"telefone\", \"estado\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_client_id_existence(spark, df_transactions, df_clients):\n",
    "    df_ids_transactions = df_transactions.select(col('cliente_id'))\n",
    "    df_ids_clients = df_clients.select(col('id'))\n",
    "    df_new_clients = df_ids_transactions.join(df_ids_clients, df_ids_transactions.cliente_id == df_ids_clients.id, \"leftanti\")\n",
    "    df_new_clients = df_new_clients.distinct()\n",
    "    df_new_clients = df_new_clients.withColumnRenamed(\"cliente_id\", \"id\")\n",
    "    df_new_clients = df_new_clients.withColumn('nome', lit('Não localizado'))\n",
    "    df_new_clients = df_new_clients.withColumn('sobrenome', lit('Não localizado'))\n",
    "    df_new_clients = df_new_clients.withColumn('email', lit('Não localizado'))\n",
    "    df_new_clients = df_new_clients.withColumn('data_cadastro', lit('1900-01-01 00:00:00').cast('timestamp'))\n",
    "    df_new_clients = df_new_clients.withColumn('telefone', lit('Não localizado'))\n",
    "    df_new_clients = df_new_clients.withColumn('estado', lit('Não localizado'))\n",
    "    df_clients = df_clients.unionAll(df_new_clients)\n",
    "    return df_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_df_in_out(df_transactions_in, df_transactions_out):\n",
    "    df_transactions_in = df_transactions_in.withColumn('tipo_transacao', lit('IN'))\n",
    "    df_transactions_out = df_transactions_out.withColumn('tipo_transacao', lit('OUT'))\n",
    "    df_transactions = df_transactions_in.unionAll(df_transactions_out)\n",
    "    return df_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection_database():\n",
    "    load_dotenv()\n",
    "    server_name = os.environ[\"server_name\"]\n",
    "    database_name = os.environ[\"database_name\"]\n",
    "    username = os.environ[\"username\"]\n",
    "    password = os.environ[\"password\"]\n",
    "\n",
    "    connection_string = f\"Driver={{ODBC Driver 18 for SQL Server}};Server=tcp:{server_name},1433;Database={database_name};Uid={username};Pwd={password};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
    "    return pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_clients(conn, df):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'clientes'\")\n",
    "    if cursor.fetchone()[0] == 0:\n",
    "        create_table_query = '''CREATE TABLE clientes (\n",
    "                                    id INTEGER PRIMARY KEY,\n",
    "                                    nome VARCHAR(255),\n",
    "                                    sobrenome VARCHAR(255),\n",
    "                                    email VARCHAR(255),\n",
    "                                    data_hora_cadastro DATETIME,\n",
    "                                    telefone VARCHAR(255),\n",
    "                                    estado VARCHAR(255)\n",
    "                                    );'''\n",
    "\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        print(\"Tabela clientes criada com sucesso!\")\n",
    "        insert_df_into_db(conn, df, \"clientes\")\n",
    "    else:\n",
    "        print(\"A tabela clientes já está no banco de dados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_df_into_db(conn, df, table_name):\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Inserindo os dados na tabela...\")\n",
    "    try:\n",
    "        columns = \",\".join(df.columns)\n",
    "        placeholders = \",\".join(\"?\" for _ in df.columns)\n",
    "        df = df.rdd.collect()\n",
    "\n",
    "        for values in df:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\", values)\n",
    "            cursor.commit()\n",
    "        print(\"Os dados foram inseridos com sucesso na tabela.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao inserir os dados na tabela: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_transactions(conn, df, name_table):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{name_table}'\")\n",
    "    if cursor.fetchone()[0] == 0:\n",
    "        create_table_query = f\"CREATE TABLE {name_table} (\\\n",
    "                                id INTEGER PRIMARY KEY,\\\n",
    "                                cliente_id INTEGER REFERENCES clientes (id),\\\n",
    "                                valor DECIMAL(10,2),\\\n",
    "                                data_hora DATETIME,\\\n",
    "                            );\"\n",
    "\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        print(f\"Tabela {name_table} criada com sucesso!\")\n",
    "        insert_df_into_db(conn, df, name_table)\n",
    "    else: \n",
    "        print(f\"A tabela {name_table} já está no banco de dados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformando os arquivos CSVs em data frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Verificando se há dados não informados nas colunas dos DataFrames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Corrigindo os dados da coluna valor dos DataFrames de transações...\n",
      "OK\n",
      "Formatando o DataFrame de clientes...\n",
      "OK\n",
      "Unindo os dados das transações em um único DataFrame...\n",
      "OK\n",
      "------------------------------\n",
      "Transações in\n",
      "+----+----------+-----+-------------------+\n",
      "|  id|cliente_id|valor|               data|\n",
      "+----+----------+-----+-------------------+\n",
      "|3120|       533|  5.0|2021-01-28 23:46:47|\n",
      "|3119|       533| 25.0|2021-01-28 23:46:47|\n",
      "|3108|       533|  5.0|2021-01-28 13:47:37|\n",
      "|3107|       533| 12.5|2021-01-28 13:47:36|\n",
      "|3106|       533| 12.5|2021-01-28 13:47:36|\n",
      "|3092|       533| 25.0|2021-01-28 13:02:57|\n",
      "|3091|       533|  5.0|2021-01-28 13:02:57|\n",
      "|3079|       533| 25.0|2021-01-23 12:44:31|\n",
      "|3078|       533|  5.0|2021-01-23 12:44:30|\n",
      "|3069|       574| 20.0|2021-01-23 00:29:52|\n",
      "|3066|        74|  7.8|2021-01-22 00:11:14|\n",
      "|3061|        74| 3.82|2021-01-20 23:05:16|\n",
      "|3060|         4| 5.58|2021-01-20 23:05:16|\n",
      "|3046|       370| 50.0|2021-01-20 19:07:00|\n",
      "|3030|       570| 20.0|2021-01-15 15:53:40|\n",
      "|3009|        74|  5.0|2021-01-14 11:04:03|\n",
      "|3008|        74| 12.5|2021-01-14 11:04:03|\n",
      "|3007|       533| 12.5|2021-01-14 11:04:03|\n",
      "|3000|       370| 50.0|2021-01-13 22:12:34|\n",
      "|2995|       570| 20.0|2021-01-13 14:22:24|\n",
      "+----+----------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "------------------------------\n",
      "Transações out\n",
      "+----+----------+-----+-------------------+\n",
      "|  id|cliente_id|valor|               data|\n",
      "+----+----------+-----+-------------------+\n",
      "|8607|       910|  2.0|2022-01-19 20:15:26|\n",
      "|8608|       910|  2.0|2022-01-19 20:14:56|\n",
      "|8609|       910|  2.0|2022-01-19 20:14:26|\n",
      "|8610|       910|  2.0|2022-01-19 20:13:56|\n",
      "|8612|       910|  2.0|2022-01-19 20:13:26|\n",
      "|8614|       910|  2.0|2022-01-19 20:12:56|\n",
      "|8573|       671| 10.0|2022-01-13 15:21:25|\n",
      "|8574|       671| 10.0|2022-01-13 15:20:55|\n",
      "|8575|       671|  5.0|2022-01-13 15:20:25|\n",
      "|8576|       671| 10.0|2022-01-13 15:19:55|\n",
      "|8577|       671|  2.0|2022-01-13 15:19:25|\n",
      "|8580|       671| 10.0|2022-01-13 15:18:55|\n",
      "|8581|       671|100.0|2022-01-13 15:18:25|\n",
      "|8582|       671| 15.0|2022-01-13 15:17:55|\n",
      "|8583|       671|100.0|2022-01-13 15:17:25|\n",
      "|8584|       671|  2.0|2022-01-13 15:16:55|\n",
      "|8579|       370|  7.0|2022-01-13 03:44:57|\n",
      "|8578|       370|  3.0|2022-01-13 03:37:43|\n",
      "|8572|       671|  2.0|2022-01-12 19:43:20|\n",
      "|8539|       671| 10.0|2022-01-12 17:16:29|\n",
      "+----+----------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "------------------------------\n",
      "Dados dos clientes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 173:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------------------+-------------------+----------------+--------------+\n",
      "| id|      nome|           sobrenome|               email|      data_cadastro|        telefone|        estado|\n",
      "+---+----------+--------------------+--------------------+-------------------+----------------+--------------+\n",
      "| 55|  Edmilson|            Da Silva|edmilson-da-silva...|2019-08-30 00:54:33|+55(22)2922-2626|   Mato Grosso|\n",
      "| 78|    Maxson|    Barros Do Santos|maxson-barros-do-...|2019-09-10 02:03:42|+55(22)2126-2529|   Mato Grosso|\n",
      "| 61|     Bruno|       Cesar E Silva|bruno-cesar-e-sil...|2019-08-30 01:05:21|+55(23)2528-2729|    Pernambuco|\n",
      "|106|  Dernival| Passos De Amarantes|dernival-passos-d...|2019-09-27 02:40:14|+55(29)2927-2322|     São Paulo|\n",
      "|107|      José|      Rubian De Goes|jose-rubian-de-go...|2019-09-28 13:09:43|+55(22)2023-2620|   Mato Grosso|\n",
      "|108|  Angelica|Dos Santos De Morais|angelica-dos-sant...|2019-09-28 14:48:16|+55(20)2521-3030|       Paraíba|\n",
      "|109|Alanderson|       Não informado|alanderson_109@gm...|2019-09-29 00:15:12|+55(22)2323-2426|   Mato Grosso|\n",
      "|142|    Silvio|             Gabriel|silvio-gabriel_14...|2019-10-04 12:47:21|+55(22)3026-2123|   Mato Grosso|\n",
      "|144|      José|Carlos Da Silva L...|jose-carlos-da-si...|2019-10-06 14:24:26|+55(21)2921-2127|Rio de Janeiro|\n",
      "|145|     Dante|   Hugo Ayres Câncio|dante-hugo-ayres-...|2019-10-06 16:36:49|+55(20)2421-2129|       Paraíba|\n",
      "|146|    Felipe|Emanuel Da Silva ...|felipe-emanuel-da...|2019-10-06 18:24:39|+55(25)2020-2820|         Bahia|\n",
      "| 88|  Anderson|         Albuquerque|anderson-albuquer...|2019-09-14 20:52:26|+55(29)2527-2125|     São Paulo|\n",
      "|159|     Arena|       Não informado| arena_159@gmail.com|2019-10-31 18:56:40|+55(28)2127-2623|       Roraima|\n",
      "|150|      Jean|       Não informado|  jean_150@gmail.com|2019-10-13 20:50:27|+55(30)2028-2130|      Maranhão|\n",
      "|248|    Daniel|       Não informado|daniel_248@gmail.com|2019-12-12 23:54:51|+55(22)2328-2725|   Mato Grosso|\n",
      "|156|   Ronaldo|             Rabello|ronaldo-rabello_1...|2019-10-25 17:26:36|+55(25)3026-2124|         Bahia|\n",
      "|157| Alexandre|              Santos|alexandre-santos_...|2019-10-26 16:48:48|+55(27)2825-2325|Espírito Santo|\n",
      "|160|   Genival|Rufino Neres Dos ...|genival-rufino-ne...|2019-10-31 20:13:50|+55(25)2225-2922|         Bahia|\n",
      "|152|   Bandeja|       Não informado|bandeja_152@gmail...|2019-10-16 13:03:12|+55(24)2727-2527|Rio de Janeiro|\n",
      "|153|     Dante|Hugo Brandao Ayre...|dante-hugo-branda...|2019-10-17 02:09:46|+55(30)2430-2227|      Maranhão|\n",
      "+---+----------+--------------------+--------------------+-------------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Transformando os arquivos CSVs em data frames...\")\n",
    "    df_clients = transform_csv_to_df(spark, clients, clients_schema)\n",
    "    df_transactions_in = transform_csv_to_df(spark, transactions_in, transactions_schema)\n",
    "    df_transactions_out = transform_csv_to_df(spark, transactions_out , transactions_schema)\n",
    "    print(\"OK\")\n",
    "\n",
    "    print(\"Verificando se há dados não informados nas colunas dos DataFrames...\")\n",
    "    verify_empty_data(df_clients)\n",
    "    verify_empty_data(df_transactions_in)\n",
    "    verify_empty_data(df_transactions_out)\n",
    "    print(\"OK\")\n",
    "\n",
    "    print(\"Corrigindo os dados da coluna valor dos DataFrames de transações...\")\n",
    "    df_transactions_in = correcting_data(df_transactions_in)\n",
    "    df_transactions_out = correcting_data(df_transactions_out)\n",
    "    print(\"OK\")\n",
    "\n",
    "    print(\"Formatando o DataFrame de clientes...\")\n",
    "    df_clients = add_state_column(df_clients)\n",
    "    df_clients = format_names(df_clients)\n",
    "    df_clients = verify_client_id_existence(spark, df_transactions_in, df_clients)\n",
    "    df_clients = verify_client_id_existence(spark, df_transactions_out, df_clients)\n",
    "    print(\"OK\")\n",
    "\n",
    "    print(\"Unindo os dados das transações em um único DataFrame...\")\n",
    "    dt_transactions = union_df_in_out(df_transactions_in, df_transactions_out)\n",
    "    print(\"OK\")\n",
    "      \n",
    "    print(\"-\" * 30)\n",
    "    print(\"Transações in\")\n",
    "    df_transactions_in.show()\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Transações out\")\n",
    "    df_transactions_out.show()\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Dados dos clientes\")\n",
    "    df_clients.show()\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        print(\"Conectando com o banco de dados...\")\n",
    "        conn = connection_database()\n",
    "    except Exception:\n",
    "        print(\"Não foi possivel se conectar com o banco de dados!\")\n",
    "    else:\n",
    "        print(\"Criando tabela de clientes no banco de dados!\")\n",
    "        create_table_clients(conn, df_clients)\n",
    "        \n",
    "        df_transactions_in = df_transactions_in.join(df_clients, df_clients.id == df_transactions_in.cliente_id, \"leftsemi\")\n",
    "        df_transactions_out = df_transactions_out.join(df_clients, df_clients.id == df_transactions_out.cliente_id, \"leftsemi\")\n",
    "\n",
    "        print(\"Criando as tabelas de transações no banco de dados!\")\n",
    "        create_table_transactions(conn, df_transactions_in, \"transactions_in\")\n",
    "        create_table_transactions(conn, df_transactions_out, \"transactions_out\")\n",
    "    '''\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu o seguinte erro: {e}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
